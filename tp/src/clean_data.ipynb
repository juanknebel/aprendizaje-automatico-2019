{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiando el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports necesarios para la limpieza y depuración del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = []\n",
    "\n",
    "with open('../data/training_data_2_csv_UTF.csv', 'r') as csv_file_in:\n",
    "    reader = csv.reader(csv_file_in, delimiter=',', quotechar='\"')\n",
    "    header = next(reader, None)\n",
    "    for row in reader:\n",
    "        data_raw.append(row)\n",
    "\n",
    "original_size = len(data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de las columnas válidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo original tiene 24 posiciones pero solo las primeras 20 son representativas.\n",
    "\n",
    "Lo primero que se debe hacer es quitar los últimos 4 elementos de las filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COLUMN = 20\n",
    "header = header[0:MAX_COLUMN]\n",
    "data_full = []\n",
    "\n",
    "for row in data_raw:\n",
    "    data_full.append(row[0:MAX_COLUMN])\n",
    "\n",
    "header_with_position = []\n",
    "\n",
    "for i in range(20):\n",
    "    header_with_position.append(str(i) + ' - ' + header[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de las filas y guardado en nuevo archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si alguna de las filas en la posición en la que esta definada la clase no termina ni con 1 o 0, no la tengo en cuenta.\n",
    "\n",
    "En caso de querer agregar mas filtros para descartar filas, solo se debe agregar a la función `is_bad_row(a_row)`.\n",
    "\n",
    "Para filtar columnas que no se quieren en el set datos final, se debe a agregar el número de columna en la lista `blacklist_column`.\n",
    "\n",
    "La función `clean_row(a_row)` es la encargada de limpiar la fila y normalizar algunos de sus atributos.\n",
    "Realza las siguientes acciones de limpieza:\n",
    "* Quitar, a todas las celdas, las comillas extra que puedan tener.\n",
    "* En la columna descripción (número 4) reemplaza el contenido de la misma por el valor Has content y por Has no content cuando es vacio.\n",
    "* En la columan url (número 5) reemplaza el contenido por Url cuando hay un url presente, Empty cuando el contenido es vacio y None cuando el mismo es None o null.\n",
    "* Normaliza la columna created_at y graba la cantidad de segundos desde la fecha indicada hasta el 1/1/2000. Hay tres tipos de fechas: 2/25/2015 20:11, 25/2/2015 20:11 o Sat Jun 07 19:34:43 +0000 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros originales: 2797\n",
      "Cantidad de registros depurados: 2770\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Las posiciones originales de los datos\n",
    "'0 - id'\n",
    "'1 - id_str'\n",
    "'2 - screen_name'\n",
    "'3 - location'\n",
    "'4 - description'\n",
    "'5 - url'\n",
    "'6 - followers_count'\n",
    "'7 - friends_count'\n",
    "'8 - listed_count'\n",
    "'9 - created_at'\n",
    "'10 - favourites_count'\n",
    "'11 - verified'\n",
    "'12 - statuses_count'\n",
    "'13 - lang'\n",
    "'14 - status'\n",
    "'15 - default_profile'\n",
    "'16 - default_profile_image'\n",
    "'17 - has_extended_profile'\n",
    "'18 - name'\n",
    "'19 - bot'\n",
    "'''\n",
    "\n",
    "LOCATION_POSITION = 3\n",
    "DESCRIPTION_POSITION = 4\n",
    "URL_POSITION = 5\n",
    "CREATED_POSITION = 9\n",
    "EXTENDED_PROFILE_POSITION = 17\n",
    "CLASS_POSITION = 19\n",
    "blacklist_column = [0, 1, 2, 14, 18]\n",
    "posibles_date_formats = [\"%a %b %d %H:%M:%S %z %Y\", \"%m/%d/%Y %H:%M\", \"%d/%m/%Y %H:%M\"]\n",
    "final_date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_2000=datetime.datetime.strptime(\"1/1/2000 0:00\", \"%m/%d/%Y %H:%M\")\n",
    "NULL_VALUE = 'NULL'\n",
    "\n",
    "def is_bad_row(a_row):\n",
    "    if row[CLASS_POSITION] == '1' or row[CLASS_POSITION] == '0':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def convert_string_date_to_total_seconds(a_string_date):\n",
    "    for a_format in posibles_date_formats:\n",
    "        try:\n",
    "            actual_date = datetime.datetime.strptime(a_string_date, a_format).replace(tzinfo=None)\n",
    "            difference = actual_date - date_2000\n",
    "            return difference.seconds\n",
    "        except:\n",
    "            continue\n",
    "    print (\"La fecha no pudo ser leida: \" + a_string_date)\n",
    "    return None\n",
    "\n",
    "def clean_row(a_row):\n",
    "    a_clean_row = []\n",
    "    for cell in a_row:\n",
    "        a_clean_row.append(cell.replace('\"', ''))\n",
    "    \n",
    "    if a_clean_row[LOCATION_POSITION].strip() == '':\n",
    "        a_clean_row[LOCATION_POSITION] = NULL_VALUE\n",
    "        \n",
    "    if a_clean_row[EXTENDED_PROFILE_POSITION] in ('None', 'null', None, ''):\n",
    "        a_clean_row[EXTENDED_PROFILE_POSITION] = NULL_VALUE\n",
    "    \n",
    "    if a_clean_row[DESCRIPTION_POSITION] in ('None', 'null', None):\n",
    "        a_clean_row[DESCRIPTION_POSITION] = NULL_VALUE\n",
    "    elif a_clean_row[DESCRIPTION_POSITION].strip() == '':\n",
    "        a_clean_row[DESCRIPTION_POSITION] = 'No descrition'\n",
    "    else:\n",
    "        a_clean_row[DESCRIPTION_POSITION] = 'Has description'\n",
    "    \n",
    "    if a_clean_row[URL_POSITION] in ('None', 'null', None):\n",
    "        a_clean_row[URL_POSITION] = NULL_VALUE\n",
    "    elif a_clean_row[URL_POSITION].strip() == '':\n",
    "        a_clean_row[URL_POSITION] = 'No url'\n",
    "    else:\n",
    "        a_clean_row[URL_POSITION] = 'Url'\n",
    "        \n",
    "    a_clean_row[CREATED_POSITION] = convert_string_date_to_total_seconds(a_clean_row[CREATED_POSITION])\n",
    "    \n",
    "    return a_clean_row\n",
    "\n",
    "def filter_columns(a_row):\n",
    "    a_filter_row = []\n",
    "    for position in range(MAX_COLUMN):\n",
    "        if not position in blacklist_column:\n",
    "            a_filter_row.append(a_row[position])\n",
    "    return a_filter_row\n",
    "\n",
    "csv.register_dialect('my_dialect', delimiter = ',', quotechar='\"')\n",
    "actual_size = 0\n",
    "\n",
    "with open('../data/newdata.csv', 'w') as csv_file_out:\n",
    "    writer = csv.writer(csv_file_out, dialect='my_dialect')\n",
    "    writer.writerow(filter_columns(header))\n",
    "    for row in data_full:\n",
    "        if not is_bad_row(row):\n",
    "            refined_row = clean_row(row)\n",
    "            filered_row = filter_columns(refined_row)\n",
    "            writer.writerow(filered_row)\n",
    "            actual_size = actual_size + 1\n",
    "\n",
    "print (\"Cantidad de registros originales: \" + str(original_size))\n",
    "print (\"Cantidad de registros depurados: \" + str(actual_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
